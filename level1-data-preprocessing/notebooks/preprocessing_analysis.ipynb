{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea47e54",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # House Price Prediction - Data Preprocessing\n",
    "# ## Task 1: Data Preprocessing for Machine Learning\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Import Required Libraries\n",
    "\n",
    "# %%\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 1. Load and Explore the Dataset\n",
    "\n",
    "# %%\n",
    "# Load the dataset\n",
    "df = pd.read_csv('../data/house_prediction.csv', header=None, delim_whitespace=True)\n",
    "\n",
    "# Add column names (based on Boston Housing dataset)\n",
    "column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', \n",
    "                'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "df.columns = column_names\n",
    "\n",
    "# Display basic information\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nDataset Info:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\nBasic Statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 2. Check for Missing Data\n",
    "\n",
    "# %%\n",
    "# Check for missing values\n",
    "missing_data = df.isnull().sum()\n",
    "print(\"Missing values in each column:\")\n",
    "print(missing_data[missing_data > 0] if any(missing_data > 0) else \"No missing values found!\")\n",
    "\n",
    "# Visualize missing data\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(df.isnull(), cbar=False, yticklabels=False, cmap='viridis')\n",
    "plt.title('Missing Data Heatmap')\n",
    "plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 3. Handle Missing Data (if any)\n",
    "\n",
    "# %%\n",
    "# Strategy 1: Fill with mean/median\n",
    "df_mean_filled = df.copy()\n",
    "for col in df_mean_filled.columns:\n",
    "    if df_mean_filled[col].isnull().any():\n",
    "        df_mean_filled[col].fillna(df_mean_filled[col].mean(), inplace=True)\n",
    "        print(f\"Filled {col} with mean: {df_mean_filled[col].mean():.2f}\")\n",
    "\n",
    "# Strategy 2: Drop rows with missing values\n",
    "df_dropped = df.dropna()\n",
    "\n",
    "# Compare shapes\n",
    "print(f\"\\nOriginal shape: {df.shape}\")\n",
    "print(f\"After dropping missing values: {df_dropped.shape}\")\n",
    "print(f\"After filling with mean: {df_mean_filled.shape}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 4. Check for Categorical Variables\n",
    "\n",
    "# %%\n",
    "# Check data types\n",
    "print(\"Data types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"\\nCategorical columns: {categorical_cols if categorical_cols else 'None found'}\")\n",
    "\n",
    "# Check unique values in each column\n",
    "print(\"\\nUnique values in each column:\")\n",
    "for col in df.columns:\n",
    "    print(f\"{col}: {df[col].nunique()} unique values\")\n",
    "\n",
    "# For this dataset, CHAS and RAD might be categorical\n",
    "categorical_cols_to_encode = ['CHAS', 'RAD']\n",
    "print(f\"\\nColumns to encode: {categorical_cols_to_encode}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 5. Encode Categorical Variables\n",
    "\n",
    "# %%\n",
    "# Create a copy for encoding\n",
    "df_encoded = df.copy()\n",
    "\n",
    "# Method 1: One-Hot Encoding\n",
    "df_one_hot = pd.get_dummies(df_encoded, columns=['CHAS', 'RAD'], prefix=['CHAS', 'RAD'])\n",
    "print(\"After One-Hot Encoding:\")\n",
    "print(f\"Shape: {df_one_hot.shape}\")\n",
    "print(df_one_hot.head())\n",
    "\n",
    "# Method 2: Label Encoding (for ordinal data)\n",
    "df_label_encoded = df_encoded.copy()\n",
    "label_encoder = LabelEncoder()\n",
    "df_label_encoded['CHAS'] = label_encoder.fit_transform(df_label_encoded['CHAS'])\n",
    "df_label_encoded['RAD'] = label_encoder.fit_transform(df_label_encoded['RAD'])\n",
    "print(\"\\nAfter Label Encoding:\")\n",
    "print(df_label_encoded[['CHAS', 'RAD']].head())\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 6. Check for Outliers\n",
    "\n",
    "# %%\n",
    "# Visualize distributions and outliers\n",
    "fig, axes = plt.subplots(3, 5, figsize=(20, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(df_encoded.columns):\n",
    "    if idx < len(axes):\n",
    "        axes[idx].boxplot(df_encoded[col])\n",
    "        axes[idx].set_title(col)\n",
    "        axes[idx].set_ylabel('Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Use IQR method to detect outliers\n",
    "def detect_outliers_iqr(data, column):\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
    "    return len(outliers), lower_bound, upper_bound\n",
    "\n",
    "print(\"Outlier Detection using IQR method:\")\n",
    "print(\"-\" * 50)\n",
    "for col in df_encoded.columns:\n",
    "    outlier_count, lb, ub = detect_outliers_iqr(df_encoded, col)\n",
    "    print(f\"{col}: {outlier_count} outliers (bounds: {lb:.2f} - {ub:.2f})\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 7. Normalize/Standardize Numerical Features\n",
    "\n",
    "# %%\n",
    "# Separate features and target\n",
    "X = df_encoded.drop('MEDV', axis=1)\n",
    "y = df_encoded['MEDV']\n",
    "\n",
    "print(\"Features shape:\", X.shape)\n",
    "print(\"Target shape:\", y.shape)\n",
    "\n",
    "# Method 1: Standardization (Z-score normalization)\n",
    "scaler_standard = StandardScaler()\n",
    "X_standardized = scaler_standard.fit_transform(X)\n",
    "X_standardized = pd.DataFrame(X_standardized, columns=X.columns)\n",
    "\n",
    "print(\"\\nAfter Standardization (mean=0, std=1):\")\n",
    "print(\"Mean values:\")\n",
    "print(X_standardized.mean().round(2))\n",
    "print(\"\\nStd values:\")\n",
    "print(X_standardized.std().round(2))\n",
    "\n",
    "# Method 2: Min-Max Normalization\n",
    "scaler_minmax = MinMaxScaler()\n",
    "X_normalized = scaler_minmax.fit_transform(X)\n",
    "X_normalized = pd.DataFrame(X_normalized, columns=X.columns)\n",
    "\n",
    "print(\"\\nAfter Min-Max Normalization (range 0-1):\")\n",
    "print(\"Min values:\")\n",
    "print(X_normalized.min())\n",
    "print(\"\\nMax values:\")\n",
    "print(X_normalized.max())\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 8. Split Dataset into Training and Testing Sets\n",
    "\n",
    "# %%\n",
    "# Split the data (using standardized features)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_standardized, y, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "print(\"Dataset Split:\")\n",
    "print(f\"Training set size: {X_train.shape}\")\n",
    "print(f\"Testing set size: {X_test.shape}\")\n",
    "print(f\"Training target size: {y_train.shape}\")\n",
    "print(f\"Testing target size: {y_test.shape}\")\n",
    "\n",
    "# Check distribution of target in train and test\n",
    "print(\"\\nTarget variable statistics:\")\n",
    "print(\"Training set - MEDV:\")\n",
    "print(y_train.describe())\n",
    "print(\"\\nTesting set - MEDV:\")\n",
    "print(y_test.describe())\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 9. Save Processed Data\n",
    "\n",
    "# %%\n",
    "# Save the processed datasets\n",
    "X_train.to_csv('../output/X_train.csv', index=False)\n",
    "X_test.to_csv('../output/X_test.csv', index=False)\n",
    "y_train.to_csv('../output/y_train.csv', index=False)\n",
    "y_test.to_csv('../output/y_test.csv', index=False)\n",
    "\n",
    "# Save the full processed dataset\n",
    "full_processed = pd.concat([X_standardized, y], axis=1)\n",
    "full_processed.to_csv('../output/house_prices_processed.csv', index=False)\n",
    "\n",
    "print(\"All processed data saved successfully!\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 10. Summary Report\n",
    "\n",
    "# %%\n",
    "# Create a preprocessing summary\n",
    "summary = {\n",
    "    'Original Shape': df.shape,\n",
    "    'Processed Shape': full_processed.shape,\n",
    "    'Features': list(X.columns),\n",
    "    'Target': 'MEDV',\n",
    "    'Missing Values Handled': 'No missing values found' if missing_data.sum() == 0 else 'Filled with mean',\n",
    "    'Categorical Encoding': 'One-Hot Encoding used',\n",
    "    'Scaling Method': 'Standardization (Z-score)',\n",
    "    'Train-Test Split': f'80-20 split',\n",
    "    'Training Samples': len(X_train),\n",
    "    'Testing Samples': len(X_test)\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PREPROCESSING SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "for key, value in summary.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
